{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:32px; font-family: 'Arial\">**Micrograd - Backpropagation Engine**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:22px; font-family: 'Arial'\">Micrograd is a lightweight Python library that focuses on automatic differentiation, a key technique for training neural networks through backpropagation. It helps compute gradients automatically, making it easier to optimize neural networks by adjusting weights and biases based on the error (loss) calculated during training. Micrograd is minimalist and designed for efficiency, offering foundational capabilities for implementing custom neural network architectures</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, inputs =(), oper=\"\",label=\"\"):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        # backward - updates the gradients of the inputs( objects used to create) of the current object\n",
    "        self.backward = lambda:None \n",
    "        # inputs - contains the inputs(or objects) which are use to create the current object\n",
    "        self.inputs = set(inputs)\n",
    "        # oper - mathematical operation used to create the current object\n",
    "        self.oper = oper \n",
    "        \"\"\" inputs(or objects) which are used to create the new\n",
    "            object are being stored in order to enable\n",
    "            'Backwardpass' \"\"\"\n",
    "    \n",
    "    # string representation of a class object/instance\n",
    "    def __repr__(self): \n",
    "        return f\"(Value={self.data})\"\n",
    "    \n",
    "    # self+other = self.__add__(other)\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        new_object = Value(data=(self.data + other.data), \n",
    "                           inputs= (self, other), \n",
    "                           oper=\"+\")\n",
    "        def backward():\n",
    "            self.grad += (1.0)*(new_object.grad)\n",
    "            other.grad += (1.0)*(new_object.grad)\n",
    "            \n",
    "        new_object.backward = backward\n",
    "        return new_object\n",
    "    \n",
    "    # invoked when __add__() doesn't works that is object is second operand (eg- 2+a)\n",
    "    def __radd__(self,other):\n",
    "        return self+other\n",
    "    \n",
    "    # self-other = self.__sub__(other)\n",
    "    def __sub__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        new_object = Value(data=(self.data - other.data),\n",
    "                           inputs= (self, other),\n",
    "                           oper=\"-\")\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (1.0)*(new_object.grad)\n",
    "            other.grad += (-1.0)*(new_object.grad)\n",
    "        \n",
    "        new_object.backward = backward\n",
    "        return new_object\n",
    "    \n",
    "    # invoked when __sub__() doesn't works that is object is second operand (eg- 2-a)\n",
    "    def __rsub__(self,other): \n",
    "        return (-self)+other\n",
    "    \n",
    "    # self*other = self.__mul__(other)\n",
    "    def __mul__(self,other): \n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        new_object = Value(data=(self.data * other.data), \n",
    "                           inputs= (self, other), \n",
    "                           oper=\"*\") \n",
    "        def backward():\n",
    "            self.grad += (other.data)*(new_object.grad)\n",
    "            other.grad += (self.data)*(new_object.grad)\n",
    "        \n",
    "        new_object.backward = backward\n",
    "        return new_object\n",
    "\n",
    "    # invoked when __add__() doesn't works that is object is second operand (eg- 2*a)\n",
    "    def __rmul__(self,other): \n",
    "        return self*other\n",
    "    \n",
    "    # self/div = self.__div__(other)\n",
    "    def __truediv__(self,other): \n",
    "        return self*(other**-1)\n",
    "    \n",
    "    # -self = self.__neg__()\n",
    "    def __neg__(self):\n",
    "        return self*(-1)\n",
    "    \n",
    "    # self**other = self.__pow__(other)\n",
    "    def __pow__(self,other): \n",
    "        assert isinstance(other, (int, float)), \"Error: only supports int/float for power\"\n",
    "        new_object = Value(data=(self.data**other),\n",
    "                           inputs=(self,),\n",
    "                           oper=f\"**{other}\")\n",
    "        \n",
    "        # dL/dx = (dx^p/dx) * (dL/dx^p)\n",
    "        def backward():\n",
    "           self.grad += (other)*(self.data**(other-1))*(new_object.grad) \n",
    "        new_object.backward = backward \n",
    "        return new_object\n",
    "        \n",
    "    \n",
    "    def tanh(self):\n",
    "        tan_h = (math.exp(2*self.data)-1)/(math.exp(2*self.data)+1)\n",
    "        new_object = Value(data=(tan_h),\n",
    "                           inputs=(self,),\n",
    "                           oper=\"tanh\")\n",
    "        def backward():\n",
    "           self.grad += (1-tan_h**2)*(new_object.grad)\n",
    "        new_object.backward = backward \n",
    "        return new_object\n",
    "    \n",
    "    def exp(self):\n",
    "        new_object = Value(data=(math.exp(self.data)),\n",
    "                           inputs=(self,),\n",
    "                           oper=\"exp\")\n",
    "        def backward():\n",
    "            self.grad += (new_object.data)*(new_object.grad)\n",
    "        new_object.backward = backward\n",
    "        return new_object\n",
    "\n",
    "    def log(self):\n",
    "        new_object = Value(data=(math.log(self.data)),\n",
    "                           inputs=(self,),\n",
    "                           oper=\"log\")\n",
    "        def backward():\n",
    "            self.grad += (1.0/self.data)*(new_object.grad)\n",
    "        new_object.backward = backward\n",
    "        return new_object\n",
    "    \n",
    "    def backward_pass(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        # topological sort as our neural network is DAG\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for input in node.inputs:\n",
    "                    build_topo(input) # DFS\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
