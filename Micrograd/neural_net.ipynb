{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:32px; font-family: 'Arial\">**Building Neural Network**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:22px; font-family: 'Arial'\">This code provides the foundational building blocks to create a basic neural network entirely from scratch. It allows you to define and configure the structure of the neural network, including layers of neurons, their connections, and how data flows through them during both training and inference. By using classes Neuron, Layer, and MLP, you can construct and customize your neural network architecture, making it suitable for various tasks in machine learning and deep learning</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from micrograd import Value\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class Module:\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    \n",
    "    def __init__(self, num_input):\n",
    "        # weight matrix for all input and num_input = no. of input to the neuron\n",
    "        self.weights = [Value(random.uniform(-1,1)) for _ in range(num_input)]\n",
    "        # bias\n",
    "        self.bias = Value(random.uniform(-1,1))\n",
    "    \n",
    "    # forward pass - computes activation = weights*input + bias\n",
    "    def __call__(self,input):\n",
    "        activation = sum((weight_i*input_i for weight_i,input_i in zip(self.weights, input)), self.bias)\n",
    "        output = activation.tanh()\n",
    "        return output\n",
    "    \n",
    "    # parameters of a neuron\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"(Neuron: inputs={len(self.weights)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "    \n",
    "    def __init__(self, num_input, num_output):\n",
    "        # creates a layer of 'num_output' neurons each takes 'num_input' input\n",
    "        self.neurons = [Neuron(num_input) for _ in range(num_output)]\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        # output matrix of size 'num_output' (num_input -> num_output)\n",
    "        output = [neuron(input) for neuron in self.neurons]\n",
    "        return output[0] if len(output) == 1 else output\n",
    "    \n",
    "    # parameters of a list i.e, parameters of neurons in the layer\n",
    "    def parameters(self):\n",
    "        layer_parameters =[]\n",
    "        for neuron in self.neurons:\n",
    "            layer_parameters.extend(neuron.parameters())\n",
    "        return layer_parameters\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"(Layer: input={len(self.neurons[0].weights)} & output={len(self.neurons)})\"\n",
    "\n",
    "class MLP(Module):\n",
    "    \n",
    "    def __init__(self, num_input,  neurons_per_layers):\n",
    "        # num_input = no. of input to the neural net\n",
    "        # neurons_per_layers = a list which comprises of no. of neurons in each layer\n",
    "        self.layers=[]\n",
    "        for i in range(len(neurons_per_layers)):\n",
    "            self.layers.append(Layer(num_input, neurons_per_layers[i]))\n",
    "            num_input = neurons_per_layers[i]\n",
    "            \n",
    "        \"\"\" In a neural network, the input layer represents the \n",
    "            input features and does not consist of layers of neurons as the \n",
    "            same way as the hidden and output layers \"\"\"\n",
    "            \n",
    "    def __call__(self, input):\n",
    "        for layer in self.layers:\n",
    "            output = layer(input)\n",
    "            input = output # output of previous layer is input to the next layer\n",
    "        return output\n",
    "    \n",
    "    # parameters of a MLP i.e, parameters of all neurons in the MLP\n",
    "    def parameters(self):\n",
    "        MLP_parameters =[]\n",
    "        for layer in self.layers:\n",
    "            MLP_parameters.extend(layer.parameters())\n",
    "        return MLP_parameters\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"(MLP: layers={len(self.layers)})\"\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
